{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape data with Python Requests and Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter Notebook! \n",
    "  \n",
    "This notebook was made for the Learno course Python for Journalists. In this module you'll learn how to automatically download data from the internet, a technique also known as scraping data. We'll be using the libraries Requests and Beautiful Soup to scrape data. Don't forget to install these libraries to your Anaconda environment. (Otherwise importing these libraries will result in an error message.) Installating these libraries needs to be done in the terminal/cmd prompt using the commands `conda install requests` and `conda install bs4`.\n",
    "\n",
    "\n",
    "## About Jupyter Notebooks and Pandas\n",
    "\n",
    "Right now you're looking at a Jupyter Notebook: an interactive, browser based programming environment. You can use these notebooks to program in R, Julia or Python - as you'll be doing later on. Read more about Jupyter Notebook in the [Jupyter Notebook Quick Start Guide](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html). \n",
    "  \n",
    "To clean up our data, we'll be using Python and Pandas. Pandas is an open-source Python library - basically an extra toolkit to go with Python - that is designed for data analysis. Pandas is flexible, easy to use and has lots of useful functions built right in. Read more about Pandas and its features in [the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "\n",
    "**Notebook shortcuts**  \n",
    "\n",
    "Within Jupyter Notebooks, there are some shortcuts you can use. If you'll be using more notebooks for your data analysis in the future, you'll remember these shortcuts soon enough. :) \n",
    "\n",
    "* `esc` will take you into command mode\n",
    "* `a` will insert cell above\n",
    "* `b` will insert cell below\n",
    "* `shift then tab` will show you the documentation for your code\n",
    "* `shift and enter` will run your cell\n",
    "* ` d d` will delete a cell\n",
    "\n",
    "**Pandas dictionary**\n",
    "\n",
    "* **dataframe**: dataframe is Pandas speak for a table with a labeled y-axis, also known as an index. (The index usually starts at 0.)\n",
    "* **series**: a series is a list, a series can be made of a single column within a dataframe.\n",
    "\n",
    "Before we dive in, a little more about Jupyter Notebooks. Every notebooks is made out of cells. A cell can either contain Markdown text - like this one - or code. In the latter you can execute your code. To see what that means, type the following command in the next cell `print(\"hello world\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the libraries we need to get started with scraping. Type `import requests`, `from bs4 import BeautifulSoup`, `import pandas as pd` and `import csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:30:36.347564Z",
     "start_time": "2018-05-10T16:30:34.864142Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's in a name**  \n",
    "Scraping is the act of automatically downloading selected data from a website. Scraping is also known as web scraping, web harvesting, web data extraction and data scraping. It can be very valueable tool for your newsroom: instead of by hand saving data from the web, you can automate and speed up the process by writing a custom Python program that downloads the information for you. \n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "**What we'll actually will be doing, when I say 'we're scraping a website':**  \n",
    "\n",
    "- tell your computer which site to visit: where do you want to download data from? \n",
    "    - we'll be using the `requests` library to requests webpages\n",
    "- save the webpage (the html-page) to the computer\n",
    "    - this too will be done with library `requests`\n",
    "- from the webpage, select the data you want to have\n",
    "    - we'll be using `BeautifulSoup` to do this\n",
    "- write the selection to a csv-file\n",
    "    - this is done with the `csv` library\n",
    "\n",
    "If there is more than 1 page where you want to get data from, you can tell your computer to move on the next page to repeat the process. But that's for another course... :) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping a website\n",
    "\n",
    "## Request webpage\n",
    "We'll be scraping a list of [Power Reactors](https://www.nrc.gov/reactors/operating/list-power-reactor-units.html) from the site of the US government. First we need to let our computer know what site we want to visit; than we can request the site using `requests.get('http://website.com')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:13:59.010151Z",
     "start_time": "2018-05-10T16:13:57.969927Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want your code to become more easily reusable, you can rewrite to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:36:08.659812Z",
     "start_time": "2018-05-10T16:36:08.547577Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `requests.get(url)` doesn't have the url in quotes; it's clear the url is a string by the quotation marks in `url = 'https://www.nrc.gov/reactors/operating/list-power-reactor-units.html'`.\n",
    "\n",
    "To check if everything went right, we can use simpy type `page`; this will return a response code. Status codes are issued by a server in response to a client's request made to the server. Read more about these code on the [wikipedia page on status codes](). Basically, if you have a 200 response code, the website loaded in just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:01.044665Z",
     "start_time": "2018-05-10T16:14:01.025593Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse HTML, select data\n",
    "Now that we've got the page, let's parse the htmlpage. To parse is just nerd speak for splitting up the original data in smaller bits. Use `BeautifulSoup(page.content, 'html.parser')`. It's pretty common when scraping, to name the first with BeautifulSoup created file 'soup'. This 'soup' variable will contain all html of the page once we're done. \n",
    "\n",
    "Off course, if you want to see what is in 'soup', you could type `print(soup)`. (Notice how there are no quotemarks, since the soup we're refering to is a variable that has data stored inside of it and it is not a string. But, when you add `soup` on a new line, the computer will also print your soup. Again: programmers like things short and sweet.\n",
    "\n",
    "Btw, the library is named after the Beautiful Soup from Alice in Wonderland... Not kidding.\n",
    "\n",
    "Now, let's make ourselves some soup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:02.754386Z",
     "start_time": "2018-05-10T16:14:02.415142Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you want to select the table from this soup. Thanks to the BeautifulSoup library, you can do this writing `soup.find('table')`, this command will look for the first `<table>` in the source code of the webpage, also known as our soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:03.730027Z",
     "start_time": "2018-05-10T16:14:03.506059Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get all rows in the table. The HTML code for rows in a table is `<tr>`. We can use the BeautifulSoup command `.find_all('tr')` to get all of these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:05.172202Z",
     "start_time": "2018-05-10T16:14:05.136868Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how with `.find_all('')` you can find all rows at once, while `.find('')` will just get you the first one of whatever it is your looking for.\n",
    "\n",
    "Since there is only 1 table on this webpage, you can either use `soup.find_all('tr')` or `table.find_all('tr')`. But if there are two or more tables on one page, the `soup.find_all('tr')` command will get you all rows, from all tables. `table.find_all('tr')` builds upon `soup.find('table')`, which will give you the **first** table; meaning that `table.find_all('tr')` will get all rows from the first table only.\n",
    "\n",
    "Don't believe me? Let's try and use `soup.find_all('tr')`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:06.554377Z",
     "start_time": "2018-05-10T16:14:06.517875Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see? Exactly the same result. Just remember; whatever assignment you give to your computer, it always refers to the data that is before the `.assignment`. Meaning `soup.find_all('tr')` looks for '`tr`'s' in `soup`, and `table.find_all('tr')` looks for `tr`s in `table`.\n",
    "\n",
    "\n",
    "Now let's say that you are especially interested in the 21st row. What do you do? Since computers start counting at zero, you should ask it for row 20 to get to see the 21st row. And since you saved all rows in the `rows` variable, you can actually say 'dear computer, give me row 20' by typing `rows[20]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:07.757940Z",
     "start_time": "2018-05-10T16:14:07.745156Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this row, do you recognize the different cells? Every cell starts with `<td>`, the HTML abbrevation for table data. You can use BeautifulSoup to look for all `td`'s in this 21st row by typing: `rows.find_all('td')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:08.924923Z",
     "start_time": "2018-05-10T16:14:08.917175Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for your information: you can even save the data from the `td`'s to a variable called cells, simply type ` cells = rows[21].find_all('td')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:10.971479Z",
     "start_time": "2018-05-10T16:14:10.965290Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to only select 1 certain row, you can probably guess how to select a data cell. Exactly, use `cells[0]` to get the first cell of `cells`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:13.255237Z",
     "start_time": "2018-05-10T16:14:13.248465Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but it doesn't look too good, does it? Let's get rid of the HTML bits and pieces around our data. Add `.text` to get the job done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:14:15.058803Z",
     "start_time": "2018-05-10T16:14:15.053231Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better, doesn't it? \n",
    "\n",
    "Unfortunately, there are too many rows in this table to get each cell like we got `Comanche Peak 105000445`. We'll going to have to automate it. Luckily this is one of the big benefits of programming. \n",
    "\n",
    "Here's what we're going to do: \n",
    "1. create an empty list to be used later\n",
    "2. extract the table from our soup, save it to the `table` variable\n",
    "3. 'loop over' our table....\n",
    "4. ...to save the data we need for each row in the table\n",
    "5. add the selected data to the list\n",
    "6. print the list\n",
    "\n",
    "At step 3 we'll 'loop over' the table. What does it mean? Well, using a for loop as its called means that we'll give our computer an assignment and have it done **for** every something. It's like your mum when she told you to treat your friends with candy: **for every one of your friend, give them a piece of candy** It's shorter than naming all your friends one by one and repeating the assignment time and time again, right? We're doing exactly the same by telling our computer: **for every row in the table, get the data inside the cells**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:37:25.348411Z",
     "start_time": "2018-05-10T16:37:25.338361Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:16:29.254893Z",
     "start_time": "2018-05-10T16:16:29.198302Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You just wrote your very first scraper - well done!\n",
    "\n",
    "## Saving the scraped data\n",
    "\n",
    "Now, off course having your data printed inside the notebook is nice. But it would be even beter to store the data in a CSV file. Remember that I explained what we'd actually be doing? Off course things are a bit more complicated; let me explain. Here's what I told you before:\n",
    "\n",
    "- tell your computer which site to visit: where do you want to download data from? \n",
    "    - we'll be using the `requests` library to requests webpages\n",
    "- save the webpage (the html-page) to the computer\n",
    "    - this too will be done with library `requests`\n",
    "- from the webpage, select the data you want to have\n",
    "    - we'll be using `BeautifulSoup` to do this\n",
    "- write the selection to a csv-file\n",
    "    - this is done with the `csv` library\n",
    "\n",
    "Here's what the code will actually do: \n",
    "1. Create a CSV file to save data in\n",
    "2. Create a CSV writer to write data with to the CSV file\n",
    "3. Tell your computer which site(s) to visit\n",
    "4. Get the webpage\n",
    "5. Select data from the webpage\n",
    "6. Write data with the CSV writer to the CSV file \n",
    "7. Save file\n",
    "\n",
    "## Save data to CSV\n",
    "\n",
    "Here's how to save data to a CSV file using the CSV library - the process involves a couple steps:\n",
    "1. create a file, open it, make sure it's 'writeable', use `open('filename.csv', 'w', encoding='utf8', newline='')`\n",
    "2. create a writer, you'll need a writer if you want to write data to the file, use `csv.writer(filename, delimiter=',')`\n",
    "3. write data to the file using the writer, use `writer.writerow([data])`\n",
    "\n",
    "Off course you can repeat step 3 as often as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:24:02.221916Z",
     "start_time": "2018-05-10T16:24:02.212331Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `ls` command you can see that a new file was created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:38:31.872133Z",
     "start_time": "2018-05-10T16:38:31.742944Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scraper\n",
    "Before we broke our essay scraper into sentences before. Now I'll be putting all these sentences together. This way, you can get a good overview of what a scraper could look like. Here's a list of what we need to do, in the exact order: \n",
    "1. Create a CSV file, open it, make it writeable\n",
    "2. Create a CSV writer to write data\n",
    "3. Write the column headers to the file\n",
    "4. Tell your computer which site(s) to visit\n",
    "5. Get the webpage\n",
    "6. Select data from the webpage\n",
    "7. Write data with the CSV writer to the CSV file \n",
    "8. Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:32:33.718331Z",
     "start_time": "2018-05-10T16:32:32.641607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check if everything worked as it's supposed to, you can import the ScrapedData.csv file as a dataframe using `pd.read_csv('filename.csv')`. Look at the dataframe to see if there's data in the file. Using `df.shape` you can even quickly check if there is as much data in the file as you'd expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:32:37.086801Z",
     "start_time": "2018-05-10T16:32:37.041336Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.shape` will give you the number of rows and columns of the dataframe. A quick way to check if really everything that should be in the CSV file is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T16:34:50.897133Z",
     "start_time": "2018-05-10T16:34:50.889696Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, happy web scraping!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
